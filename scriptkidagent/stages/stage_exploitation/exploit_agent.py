import os
from openai import OpenAI
import json
from scriptkidagent.tools.schemas import schema_execute_in_msfconsole, schema_finish_with_report, schema_execute_in_bash
from scriptkidagent.tools.tools import execute_in_msfconsole, execute_in_bash
from pwn import *


class ExploitAgent:

    def __init__(self, ip, port, vulnerability_report, **kwargs):
        """
        Initializes the ExploitAgent with target details and configuration.

        Args:
            ip (str): Target IP address.
            port (int): Target port number.
            vulnerability_report (str): Description of the vulnerability.
            **kwargs: Optional keyword arguments to override default settings.
                - model (str): OpenAI model to use (default: "gpt-4").
                - max_iterations (int): Maximum number of exploitation attempts (default: 20).
        """
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("Environment variable OPENAI_API_KEY is not set.")

        self.ip = ip
        self.port = port
        self.vulnerability_report = vulnerability_report
        self.message_history = []

        # Override default configurations with kwargs if provided
        self.model = kwargs.get("model", "gpt-4o-2024-08-06")
        self.max_iterations = kwargs.get("max_iterations", 20)

        # Define available tools with their schemas
        self.tools = [schema_execute_in_msfconsole, schema_finish_with_report, schema_execute_in_bash]
        self.sucess = False
        self.if_shell = False
        self.if_root = False
        self.capabilities = []
        self.report = {}
        self.process = process(["msfconsole", "-q", "--no-readline"],
                               env={"HOME": os.environ['HOME'], "TERM": "dumb"})  # TODO: refine the args
        self.process.recvuntil(b'> ')

    def exploit(self):
        """
        Main exploit method that orchestrates the interaction loop with the LLM.
        It initializes the conversation and iteratively interacts with the LLM
        until the exploit is completed or the maximum number of iterations is reached.

        Returns:
            str: Final exploit result or failure message.
        """
        self.report = {"capabilities": [], "if_shell": False, "if_root": False, "message_history": []}

        initial_query = (
            f"Target IP: {self.ip}, Port: {self.port}\n"
            f"Vulnerability Report:\n{self.vulnerability_report}\n"
            f"Try to exploit the vulnerability using the tools: execute_in_msfconsole, finish_with_report, execute_in_bash"
            f"When exploiting with metasploit, if your exploit attempt with some specific options fails, try to change the options accordingly using your inherit knowledges or collect more info and try again. You can also try different payloads and exploits."
            f"Note that execute_in_bash is run in the host machine rather than in the remote shell."
            f"Which means you should still use execute_in_msfconsole to execute whoami after you get the shell from metasploit"
            f"only if you have sucessfully exploited the target and got a shell, you can finish the exploit with finish_with_report, otherwise you can keep trying."
        )

        # Initialize message history with system prompt
        self.message_history.append({"role": "system", "content": initial_query})

        for iteration in range(self.max_iterations):
            response = self.next_step()

            # Check if the response includes finishing the exploit with a report
            if self.sucess:
                print(f"Exploit completed on iteration {iteration + 1}: {response['output']}")
                self.report["capabilities"] = self.capabilities
                self.report["if_shell"] = self.if_shell
                self.report["if_root"] = self.if_root
                self.report["message_history"] = self.message_history
                if self.if_shell:
                    print("Shell obtained!")
                    return self.sucess, self.report, self.process
                else:
                    print("Shell not obtained.")
                    self.process.close()
                    return self.sucess, self.report, None

        print("Max iterations reached. Exploit attempt ended.")
        self.report["capabilities"] = self.capabilities
        self.report["if_shell"] = self.if_shell
        self.report["if_root"] = self.if_root
        self.report["message_history"] = self.message_history
        return self.sucess, self.report, None

    def next_step(self):
        """
        Sends the next step query to the LLM, parses the response for tool calls,
        executes the corresponding tool if identified, and updates the message history.

        Returns:
            dict: Dictionary containing the tool used and its output, or the raw response.
        """
        response = self.query_llm()

        # If the LLM returns a function call, handle it accordingly
        if response.function_call:
            tool_name = response.function_call.name
            tool_args = json.loads(response.function_call.arguments)
            if tool_name and tool_args:
                print(f"Function call: {tool_name} with args: {tool_args}")

            if tool_name == "execute_in_msfconsole":
                command = tool_args.get("command")
                if not command:
                    print("Error: 'command' not provided for execute_in_msfconsole.")
                    self.message_history.append({"role": "user", "content": "Invalid tool input."})
                    return {"tool": None, "output": "Invalid tool input."}
                execution_result, is_blocked = execute_in_msfconsole(self.process, command)
                # Append tool execution result to message history
                self.message_history.append({"role": "user", "content": "The execution output is " + execution_result})
                print(f"Execution result: {execution_result}")
                return {"tool": tool_name,
                        "output": f"The tool {tool_name} was executed with args: {tool_args}, the output is: {execution_result}"}

            if tool_name == "execute_in_bash":
                command = tool_args.get("command")
                if not command:
                    print("Error: 'command' not provided for execute_in_bash.")
                    self.message_history.append({"role": "user", "content": "Invalid tool input."})
                    return {"tool": None, "output": "Invalid tool input."}
                execution_result = execute_in_bash(command)
                # Append tool execution result to message history
                self.message_history.append({"role": "user", "content": "The execution output is " + execution_result})
                print(f"Execution result: {execution_result}")
                return {"tool": tool_name,
                        "output": f"The tool {tool_name} was executed with args: {tool_args}, the output is: {execution_result}"}

            elif tool_name == "finish_with_report":
                self.sucess = True
                self.capabilities = tool_args.get("capabilities", [])
                self.if_shell = tool_args.get("if_shell", False)
                self.if_root = tool_args.get("if_root", False)
                print(f"Exploit completed with capabilities: {self.capabilities}, if_shell: {self.if_shell}, if_root: {self.if_root}")
                return {"tool": tool_name,
                        "output": f"exploit completed with capabilities: {self.capabilities}, if_shell: {self.if_shell}, if_root: {self.if_root}"}

        # If no function call, return the raw response
        return {"tool": None, "output": response.content}

    def query_llm(self):
        """
        Queries the LLM with the given prompt and returns its response.

        Args:
            prompt (str): The prompt to send to the LLM.

        Returns:
            dict: The LLM's response, potentially containing a function call.
        """
        try:
            client = OpenAI(api_key=self.api_key)
            response = client.chat.completions.create(
              model=self.model,
              messages=self.message_history,
              functions=self.tools,  # Pass the tool definitions to the LLM
              function_call="auto",
              # Let the LLM decide when to call a function
            )
            message = response.choices[0].message
            self.message_history.append(message)
            return message
        except Exception as e:
            error_message = f"Error querying LLM: {str(e)}"
            print(error_message)
            self.message_history.append({"role": "user", "content": error_message})
            return {"content": error_message}
